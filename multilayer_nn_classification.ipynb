{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 461,
      "metadata": {
        "id": "3dA__Py7W10c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = np.load(\"test_data.npy\")\n",
        "test_label = np.load(\"test_label.npy\")\n",
        "train_data = np.load(\"train_data.npy\")\n",
        "train_label = np.load(\"train_label.npy\")\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_label.shape)\n",
        "print(test_data.shape)\n",
        "print(test_label.shape)\n",
        "print(np.unique(train_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 463,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Activation:\n",
        "    '''\n",
        "    An Activation object for the purposes of calling Activation functions and their respective derivatives in the MLP.\n",
        "    '''\n",
        "    def __relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "  \n",
        "    def __relu_deriv(self, a):\n",
        "        return np.heaviside(a, 0)\n",
        "\n",
        "    def __gelu(self, x):\n",
        "        return x * (1.0 / (1.0 + np.exp(-1.702 * x)))\n",
        "    \n",
        "    def __gelu_deriv(self, a):\n",
        "        sigmoid = 1.0 / (1.0 + np.exp(-1.702 * a))\n",
        "        return sigmoid + a * 1.702 * sigmoid * (1 - sigmoid)\n",
        "\n",
        "    def __softmax(self, z):\n",
        "        z = np.atleast_2d(z)\n",
        "        max_z = np.max(z, axis=1)\n",
        "        z = [z[i] - max_z[i] for i in range(max_z.shape[0])]\n",
        "        z = np.array(z)\n",
        "        return np.divide(np.exp(z).T, np.sum(np.exp(z), axis=1)).T\n",
        "\n",
        "    def __softmax_deriv(self, y, y_hat):\n",
        "        return y_hat - y\n",
        "\n",
        "    def __init__(self, activation='relu'):\n",
        "        if activation == 'relu': \n",
        "            self.f = self.__relu\n",
        "            self.f_deriv = self.__relu_deriv\n",
        "        elif activation == \"softmax\":\n",
        "            self.f = self.__softmax\n",
        "            self.f_deriv = self.__softmax_deriv\n",
        "        elif activation == \"gelu\":\n",
        "            self.f = self.__gelu\n",
        "            self.f_deriv = self.__gelu_deriv\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {activation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hidden Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 464,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class HiddenLayer(object):\n",
        "    def __init__(self, \n",
        "                 n_in, \n",
        "                 n_out, \n",
        "                 activation_last_layer='relu',\n",
        "                 activation='relu',\n",
        "                 W=None,\n",
        "                 b=None,\n",
        "                 last_hidden_layer=False):\n",
        "        self.last_hidden_layer = last_hidden_layer\n",
        "        self.input = None\n",
        "        self.activation = Activation(activation).f\n",
        "        self.activation_deriv = None\n",
        "\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv = Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W = np.random.uniform(low=-np.sqrt(6. / (n_in + n_out)),\n",
        "                                   high=np.sqrt(6. / (n_in + n_out)),\n",
        "                                   size=(n_in, n_out))\n",
        "        self.b = np.zeros(n_out,)\n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        # Adam-specific moment estimates for weights and biases\n",
        "        self.m_W = np.zeros_like(self.W)\n",
        "        self.v_W = np.zeros_like(self.W)\n",
        "        self.m_b = np.zeros_like(self.b)\n",
        "        self.v_b = np.zeros_like(self.b)\n",
        "        # Momentum SGD velocity terms\n",
        "        self.v_W_sgd = np.zeros_like(self.W)\n",
        "        self.v_b_sgd = np.zeros_like(self.b)\n",
        "        self.binomial_array = np.zeros(n_out)\n",
        "\n",
        "        # Batch normalization parameters (only for hidden layers, not output)\n",
        "        self.use_batch_norm = False  # Will be set by MLP\n",
        "        self.bn_momentum = 0.9\n",
        "        self.bn_epsilon = 1e-5\n",
        "        self.gamma = np.ones(n_out)  # Scale parameter\n",
        "        self.beta = np.zeros(n_out)  # Shift parameter\n",
        "        self.running_mean = np.zeros(n_out)\n",
        "        self.running_var = np.ones(n_out)\n",
        "        self.grad_gamma = np.zeros_like(self.gamma)\n",
        "        self.grad_beta = np.zeros_like(self.beta)\n",
        "        # Adam-specific moment estimates for batch norm parameters\n",
        "        self.m_gamma = np.zeros_like(self.gamma)\n",
        "        self.v_gamma = np.zeros_like(self.gamma)\n",
        "        self.m_beta = np.zeros_like(self.beta)\n",
        "        self.v_beta = np.zeros_like(self.beta)\n",
        "        # Cache for batch normalization\n",
        "        self.bn_input = None\n",
        "        self.bn_normalized = None\n",
        "        self.bn_mean = None\n",
        "        self.bn_var = None\n",
        "\n",
        "    def batch_norm_forward(self, x, training=True):\n",
        "        if not self.use_batch_norm or self.last_hidden_layer:\n",
        "            return x\n",
        "        if training:\n",
        "            # Compute batch mean and variance\n",
        "            self.bn_mean = np.mean(x, axis=0)\n",
        "            self.bn_var = np.var(x, axis=0)\n",
        "            # Update running statistics\n",
        "            self.running_mean = self.bn_momentum * self.running_mean + (1 - self.bn_momentum) * self.bn_mean\n",
        "            self.running_var = self.bn_momentum * self.running_var + (1 - self.bn_momentum) * self.bn_var\n",
        "            # Normalize\n",
        "            self.bn_normalized = (x - self.bn_mean) / np.sqrt(self.bn_var + self.bn_epsilon)\n",
        "        else:\n",
        "            # Use running statistics for inference\n",
        "            self.bn_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.bn_epsilon)\n",
        "        # Scale and shift\n",
        "        out = self.gamma * self.bn_normalized + self.beta\n",
        "        self.bn_input = x\n",
        "        return out\n",
        "\n",
        "    def batch_norm_backward(self, delta):\n",
        "        if not self.use_batch_norm or self.last_hidden_layer:\n",
        "            return delta\n",
        "        batch_size = self.bn_input.shape[0]\n",
        "        # Gradients for gamma and beta\n",
        "        self.grad_gamma = np.sum(delta * self.bn_normalized, axis=0)\n",
        "        self.grad_beta = np.sum(delta, axis=0)\n",
        "        # Gradient of the normalized input\n",
        "        d_normalized = delta * self.gamma\n",
        "        d_var = np.sum(d_normalized * (self.bn_input - self.bn_mean) * -0.5 * (self.bn_var + self.bn_epsilon) ** (-1.5), axis=0)\n",
        "        d_mean = np.sum(d_normalized * -1 / np.sqrt(self.bn_var + self.bn_epsilon), axis=0) + d_var * np.sum(-2 * (self.bn_input - self.bn_mean), axis=0) / batch_size\n",
        "        delta = d_normalized / np.sqrt(self.bn_var + self.bn_epsilon) + d_var * 2 * (self.bn_input - self.bn_mean) / batch_size + d_mean / batch_size\n",
        "        return delta\n",
        "\n",
        "    @staticmethod\n",
        "    def dropout_forward(X, p_dropout):\n",
        "        u = np.random.binomial(1, 1 - p_dropout, size=X.shape) \n",
        "        out = X * u\n",
        "        binomial_array = u\n",
        "        return out, binomial_array\n",
        "    \n",
        "    @staticmethod\n",
        "    def dropout_backward(delta, binomial_array, layer_num):\n",
        "        delta *= nn.layers[layer_num - 1].binomial_array\n",
        "        return delta\n",
        "    \n",
        "    def forward(self, input, training=True):\n",
        "        self.input = input\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        # Apply batch normalization before activation\n",
        "        bn_output = self.batch_norm_forward(lin_output, training)\n",
        "        self.output = (\n",
        "            bn_output if self.activation is None\n",
        "            else self.activation(bn_output)\n",
        "        ) \n",
        "        if not self.last_hidden_layer and training:\n",
        "            self.output, self.binomial_array = self.dropout_forward(self.output, DROPOUT_PROB)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, delta, layer_num, output_layer=False):\n",
        "        if self.activation_deriv and not output_layer:\n",
        "            delta = delta * self.activation_deriv(self.output)\n",
        "        if not output_layer:\n",
        "            delta = self.batch_norm_backward(delta)\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        self.grad_b = np.average(delta, axis=0)\n",
        "        if layer_num != 0 and self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T)\n",
        "        if layer_num != 0:\n",
        "            delta = self.dropout_backward(delta, self.binomial_array, layer_num)\n",
        "        return delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 465,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers, activation=[None, 'gelu', 'relu', 'softmax'], weight_decay=1.0, use_batch_norm=False):\n",
        "        self.layers = []\n",
        "        self.activation = activation\n",
        "        self.weight_decay = weight_decay\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.t = 0  # Time step for Adam\n",
        "        \n",
        "        for i in range(len(layers)-1):\n",
        "            last_hidden_layer = (i == len(layers) - 2)\n",
        "            layer = HiddenLayer(layers[i], \n",
        "                                layers[i+1], \n",
        "                                activation[i], \n",
        "                                activation[i+1],\n",
        "                                last_hidden_layer=last_hidden_layer)\n",
        "            layer.use_batch_norm = self.use_batch_norm\n",
        "            self.layers.append(layer)\n",
        "            \n",
        "    def forward(self, input, training=True):\n",
        "        for layer in self.layers: \n",
        "            output = layer.forward(input, training)\n",
        "            input = output \n",
        "        return output\n",
        "\n",
        "    def CE_loss(self, y, y_hat):\n",
        "        loss = - np.nansum(y * np.log(y_hat + 1e-15))\n",
        "        loss = loss / y.shape[0] \n",
        "        loss = loss * self.weight_decay\n",
        "        delta = Activation(self.activation[-1]).f_deriv(y, y_hat)\n",
        "        return loss, delta\n",
        "\n",
        "    def backward(self, delta):\n",
        "        delta = self.layers[-1].backward(delta, len(self.layers) - 1, output_layer=True)\n",
        "        for layer_num, layer in reversed(list(enumerate(self.layers[:-1]))):\n",
        "            delta = layer.backward(delta, layer_num)\n",
        "    \n",
        "    def update(self, lr, optim_params, optim_type='adam'):\n",
        "        '''\n",
        "        Update parameters using either Adam or Momentum SGD.\n",
        "        '''\n",
        "        if optim_type == 'adam':\n",
        "            self.t += 1\n",
        "            beta1, beta2, epsilon = optim_params['beta1'], optim_params['beta2'], optim_params['epsilon']\n",
        "            for layer in self.layers:\n",
        "                # Update weights\n",
        "                layer.m_W = beta1 * layer.m_W + (1 - beta1) * layer.grad_W\n",
        "                layer.v_W = beta2 * layer.v_W + (1 - beta2) * (layer.grad_W ** 2)\n",
        "                m_hat_W = layer.m_W / (1 - beta1 ** self.t)\n",
        "                v_hat_W = layer.v_W / (1 - beta2 ** self.t)\n",
        "                layer.W -= lr * m_hat_W / (np.sqrt(v_hat_W) + epsilon)\n",
        "                # Update biases\n",
        "                layer.m_b = beta1 * layer.m_b + (1 - beta1) * layer.grad_b\n",
        "                layer.v_b = beta2 * layer.v_b + (1 - beta2) * (layer.grad_b ** 2)\n",
        "                m_hat_b = layer.m_b / (1 - beta1 ** self.t)\n",
        "                v_hat_b = layer.v_b / (1 - beta2 ** self.t)\n",
        "                layer.b -= lr * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n",
        "                # Update batch norm parameters if enabled\n",
        "                if layer.use_batch_norm and not layer.last_hidden_layer:\n",
        "                    layer.m_gamma = beta1 * layer.m_gamma + (1 - beta1) * layer.grad_gamma\n",
        "                    layer.v_gamma = beta2 * layer.v_gamma + (1 - beta2) * (layer.grad_gamma ** 2)\n",
        "                    m_hat_gamma = layer.m_gamma / (1 - beta1 ** self.t)\n",
        "                    v_hat_gamma = layer.v_gamma / (1 - beta2 ** self.t)\n",
        "                    layer.gamma -= lr * m_hat_gamma / (np.sqrt(v_hat_gamma) + epsilon)\n",
        "                    layer.m_beta = beta1 * layer.m_beta + (1 - beta1) * layer.grad_beta\n",
        "                    layer.v_beta = beta2 * layer.v_beta + (1 - beta2) * (layer.grad_beta ** 2)\n",
        "                    m_hat_beta = layer.m_beta / (1 - beta1 ** self.t)\n",
        "                    v_hat_beta = layer.v_beta / (1 - beta2 ** self.t)\n",
        "                    layer.beta -= lr * m_hat_beta / (np.sqrt(v_hat_beta) + epsilon)\n",
        "        elif optim_type == 'sgd_momentum':\n",
        "            momentum = optim_params['momentum']\n",
        "            for layer in self.layers:\n",
        "                layer.v_W_sgd = momentum * layer.v_W_sgd + lr * layer.grad_W\n",
        "                layer.W -= layer.v_W_sgd\n",
        "                layer.v_b_sgd = momentum * layer.v_b_sgd + lr * layer.grad_b\n",
        "                layer.b -= layer.v_b_sgd\n",
        "                if layer.use_batch_norm and not layer.last_hidden_layer:\n",
        "                    layer.gamma -= lr * layer.grad_gamma\n",
        "                    layer.beta -= lr * layer.grad_beta\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer type: {optim_type}\")\n",
        "              \n",
        "    def fit(self, X, y, learning_rate=0.001, epochs=100, optim_params=None, optim_type='adam', batch_size=1):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        training_loss = []\n",
        "        training_accuracy = []\n",
        "        testing_accuracy = []\n",
        "\n",
        "        num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
        "            \n",
        "        for k in range(epochs):\n",
        "            loss = np.zeros(num_batches) \n",
        "            current_idx = 0 \n",
        "            X, y = Utils.shuffle(X, y)\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                y_hat = self.forward(X[current_idx : current_idx + batch_size, :], training=True)\n",
        "                loss[batch_idx], delta = self.CE_loss(y[current_idx : current_idx + batch_size], y_hat)\n",
        "                self.backward(delta)\n",
        "                self.update(learning_rate, optim_params, optim_type)\n",
        "\n",
        "                if (current_idx + batch_size) > X.shape[0]:\n",
        "                    batch_size = X.shape[0] - current_idx\n",
        "                current_idx += batch_size\n",
        "\n",
        "            test_predict = self.predict(test_df.X)\n",
        "            train_predict = self.predict(train_df.X)\n",
        "            test_predict = test_df.decode(test_predict)\n",
        "            train_predict = train_df.decode(train_predict)\n",
        "            test_accuracy = np.sum(test_predict == test_label[:, 0]) / test_predict.shape[0]\n",
        "            train_accuracy = np.sum(train_predict == train_label[:, 0]) / train_predict.shape[0]\n",
        "\n",
        "            training_loss.append(np.mean(loss))\n",
        "            training_accuracy.append(train_accuracy)\n",
        "            testing_accuracy.append(test_accuracy)\n",
        "\n",
        "            output_dict = {'Training Loss': training_loss, 'Training Accuracy': training_accuracy, 'Testing Accuracy': testing_accuracy}\n",
        "\n",
        "            print(f'Epoch {k+1}/{epochs} has been trained with Train Loss: {str(round(training_loss[-1], 4))}, Training Accuracy: {str(round(training_accuracy[-1] * 100, 4))}% and Testing Accuracy: {str(round(testing_accuracy[-1] * 100, 4))}%.')\n",
        "         \n",
        "        return output_dict\n",
        "            \n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = [self.forward(x[i:i+1, :], training=False) for i in range(x.shape[0])]\n",
        "        output = np.array(output).squeeze()\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.predictions = None\n",
        "\n",
        "    def normalize(self):     \n",
        "        norm_data = (self.X - np.min(self.X))/(np.max(self.X) - np.min(self.X))\n",
        "        self.X = norm_data\n",
        "\n",
        "    def standardize(self):\n",
        "        self.X = (self.X - np.mean(self.X)) / np.std(self.X)\n",
        "\n",
        "    @staticmethod\n",
        "    def label_encode(label_vector):\n",
        "        num_classes = np.unique(label_vector).size\n",
        "        encoded_label_vector = []\n",
        "        for label in label_vector:\n",
        "            encoded_label = np.zeros(num_classes)\n",
        "            encoded_label[int(label)] = 1\n",
        "            encoded_label_vector.append(encoded_label)\n",
        "        encoded_label_vector = np.array(encoded_label_vector) \n",
        "        return encoded_label_vector\n",
        "    \n",
        "    @staticmethod\n",
        "    def decode(prediction_matrix):\n",
        "        decoded_predictions = np.zeros(prediction_matrix.shape[0])\n",
        "        for prediction_idx, prediction_vector in enumerate(prediction_matrix):\n",
        "            decoded_predictions[prediction_idx] = int(np.argmax(prediction_vector))\n",
        "        return decoded_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Utils:\n",
        "    @staticmethod\n",
        "    def shuffle(X, y):\n",
        "        shuffled_idx = np.arange(X.shape[0])\n",
        "        np.random.shuffle(shuffled_idx)\n",
        "        X = X[shuffled_idx]\n",
        "        y = y[shuffled_idx]\n",
        "        return X, y\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_confusion_mat(df):\n",
        "        confusion_mat = pd.DataFrame(0, index=np.unique(df.y), columns=np.unique(df.y))\n",
        "        for i in range(0, len(df.y)):\n",
        "            confusion_mat[int(df.y[i])].iloc[int(df.predictions[i])] += 1\n",
        "        return confusion_mat\n",
        "    \n",
        "    @staticmethod\n",
        "    def confusion_mat_measures(confusion_matrix):\n",
        "        scores_df = pd.DataFrame(0, index=confusion_matrix.index, columns=['Precision', 'Recall', 'F1'])\n",
        "        for i in confusion_matrix.index:\n",
        "            TP = confusion_matrix[i][i]\n",
        "            FN = np.array(confusion_matrix[i].iloc[0:i].values.tolist() + confusion_matrix[i].iloc[i+1:].values.tolist()).sum()\n",
        "            FP = np.array(confusion_matrix.iloc[i][0:i].values.tolist() + confusion_matrix.iloc[i][i + 1:].values.tolist()).sum()\n",
        "            TN = confusion_matrix.sum().sum() - TP - FN - FP\n",
        "            Precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "            Recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "            F1 = (2 * Precision * Recall) / (Precision + Recall) if (Precision + Recall) != 0 else 0\n",
        "            scores_df.loc[i, 'Precision'] = Precision\n",
        "            scores_df.loc[i, 'Recall'] = Recall\n",
        "            scores_df.loc[i, 'F1'] = F1\n",
        "        scores_df.index.name = 'Label'\n",
        "        return scores_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 468,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/500 has been trained with Train Loss: 1.632, Training Accuracy: 48.448% and Testing Accuracy: 47.66%.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[468], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Perform fitting using the training dataset\u001b[39;00m\n\u001b[1;32m     32\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 33\u001b[0m trial1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfit(train_df\u001b[38;5;241m.\u001b[39mX, train_df\u001b[38;5;241m.\u001b[39my, learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE, epochs\u001b[38;5;241m=\u001b[39mEPOCHS, optim_params\u001b[38;5;241m=\u001b[39mOPTIM_PARAMS, optim_type\u001b[38;5;241m=\u001b[39mOPTIM_TYPE, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m     34\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============= Model Build Done =============\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[465], line 107\u001b[0m, in \u001b[0;36mMLP.fit\u001b[0;34m(self, X, y, learning_rate, epochs, optim_params, optim_type, batch_size)\u001b[0m\n\u001b[1;32m    104\u001b[0m     current_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    106\u001b[0m test_predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(test_df\u001b[38;5;241m.\u001b[39mX)\n\u001b[0;32m--> 107\u001b[0m train_predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(train_df\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m    108\u001b[0m test_predict \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdecode(test_predict)\n\u001b[1;32m    109\u001b[0m train_predict \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mdecode(train_predict)\n",
            "Cell \u001b[0;32mIn[465], line 125\u001b[0m, in \u001b[0;36mMLP.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    124\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x)\n\u001b[0;32m--> 125\u001b[0m     output \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, :], training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m    126\u001b[0m     output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(output)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "Cell \u001b[0;32mIn[465], line 125\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    124\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x)\n\u001b[0;32m--> 125\u001b[0m     output \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, :], training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m    126\u001b[0m     output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(output)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "Cell \u001b[0;32mIn[465], line 21\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: \n\u001b[0;32m---> 21\u001b[0m         output \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m, training)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m output \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "Cell \u001b[0;32mIn[464], line 104\u001b[0m, in \u001b[0;36mHiddenLayer.forward\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 104\u001b[0m     lin_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Apply batch normalization before activation\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     bn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm_forward(lin_output, training)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Instantiating our data and pre-processing it as required\n",
        "train_df = Preprocessing(train_data, train_label)\n",
        "test_df = Preprocessing(test_data, test_label)\n",
        "\n",
        "# Standardize X matrix (features)\n",
        "train_df.standardize()\n",
        "test_df.standardize()\n",
        "\n",
        "# Perform one-hot encoding for our label vector (ONLY ON TRAIN)\n",
        "train_df.y = train_df.label_encode(train_df.y)\n",
        "\n",
        "# Hyperparameters\n",
        "LAYER_NEURONS = [128, 128, 64, 10]\n",
        "LAYER_ACTIVATION_FUNCS = [None, 'relu', 'gelu', 'softmax']\n",
        "LEARNING_RATE = 0.001  # Suitable for Adam; adjust for SGD if used\n",
        "EPOCHS = 300\n",
        "DROPOUT_PROB = 0.3\n",
        "ADAM_PARAMS = {'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
        "SGD_OPTIM = {'momentum': 0.9}  # Momentum parameter for SGD\n",
        "OPTIM_TYPE = 'adam'  # Can be 'adam' or 'sgd_momentum'\n",
        "BATCH_SIZE = 32\n",
        "WEIGHT_DECAY = 0.98\n",
        "USE_BATCH_NORM = True\n",
        "\n",
        "# Select optimizer parameters based on type\n",
        "OPTIM_PARAMS = ADAM_PARAMS if OPTIM_TYPE == 'adam' else SGD_OPTIM\n",
        "\n",
        "# Instantiate the multi-layer neural network\n",
        "nn = MLP(LAYER_NEURONS, LAYER_ACTIVATION_FUNCS, weight_decay=WEIGHT_DECAY, use_batch_norm=USE_BATCH_NORM)\n",
        "\n",
        "# Perform fitting using the training dataset\n",
        "t0 = time.time()\n",
        "trial1 = nn.fit(train_df.X, train_df.y, learning_rate=LEARNING_RATE, epochs=EPOCHS, optim_params=OPTIM_PARAMS, optim_type=OPTIM_TYPE, batch_size=BATCH_SIZE)\n",
        "t1 = time.time()\n",
        "print(f\"============= Model Build Done =============\")\n",
        "print(f\"Time taken to build model: {round(t1 - t0, 4)} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize = (20, 10))\n",
        "ax[0].plot(trial1['Training Loss'])\n",
        "ax[0].title.set_text(\"Cross-Entropy Loss over Epoch\")\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[1].plot(trial1['Training Accuracy'], label = \"Training Accuracy\")\n",
        "ax[1].plot(trial1['Testing Accuracy'], label = \"Testing Accuracy\")\n",
        "ax[1].title.set_text(\"Training & Testing Accuracy over Epoch\")\n",
        "ax[1].set_xlabel(\"Epoch\")\n",
        "ax[1].set_ylabel(\"Accuracy\")\n",
        "ax[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform prediction of the test dataset\n",
        "test_df.predictions = nn.predict(test_df.X)\n",
        "train_df.predictions = nn.predict(train_df.X)\n",
        "test_df.predictions = test_df.decode(test_df.predictions)\n",
        "train_df.predictions = train_df.decode(train_df.predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Confusion Matrix\n",
        "CM = Utils.create_confusion_mat(test_df)\n",
        "CM  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Performance Metrics/Measures\n",
        "measures = Utils.confusion_mat_measures(CM)\n",
        "measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy & Performance Metrics\n",
        "test_accuracy = np.sum(test_df.predictions == test_df.y[:, 0]) / test_df.predictions.shape[0]\n",
        "train_accuracy = np.sum(train_df.predictions == train_label[:, 0]) / train_df.predictions.shape[0]\n",
        "F1_avg = measures['F1'].mean()\n",
        "CELoss = trial1['Training Loss'][-1]\n",
        "print(f'Final Cross-Entropy Training Loss: {round(CELoss, 4)}.')\n",
        "print(f'Final Train accuracy: {round(train_accuracy * 100, 4)}%.')\n",
        "print(f'Final Test accuracy: {round(test_accuracy * 100, 4)}%.')\n",
        "print(f'Final Average F1 Score: {round(F1_avg, 4)}.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
